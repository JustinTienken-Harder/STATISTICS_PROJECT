---
title: "Hostel Ratings And Prices"
author: "Ashley, Manda, Justin"
date: "12/10/2019"
output: 
  ioslides_presentation:
    widescreen: true 
    smaller: true 
---


```{r, include = FALSE}

library(tidyverse) # For data processing and visualization
library(data.table) # For creating tables
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(plotly)
library(vwr)
library(DT)
library(gridExtra)
#install.packages("plotrix")
library(plotrix)
library(dplyr)
library(tidyr)
library(beeswarm)
library(ggplot2)
#library(wesanderson)
library(gridExtra)
library(plyr)
library(dplyr)
library(DT)
#install.packages("effsize")
library(effsize)

```

```{r, include = FALSE}
spot_color = "#9ecae1" 
text_color = "#525252"
home.dir <- getwd()
data.dir <- './raw_data'
# viz.dir <- '.'
data <- read.csv(file.path(home.dir, data.dir, "Hotel_Japan.csv"), na = c("NULL", "", "NA"))
```




## Introducing the Data 
- We obtained our sample of 342 Japanese hostels from [kaggle user Koki Ando](https://www.kaggle.com/koki25ando/hostel-world-dataset/version/3)  
- In the first sections of project we will use descriptive statistics to describe our sample  
- For Parts two through five we will be conducting statistical inference on the population of Japanese Hostels on Hostelworld.com  
- Our sample includes 12 variables, however, these were our variables of interest:  
  + Ratings: Summarys, Text, and Specific based on experiencial qualities  
  
  + City: Tokyo, Osaka, Kyoto, Hiroshima, Fukuoka-City  
  
  + Lowest Price: lowest price for a one night stay  
  + Value for money: User rating of whether their stay was worth the money 
```{r, echo = FALSE, warning = FALSE, include = FALSE}
quality.func <- function(data) {
  
  quality.report <- data_frame()

  cnt.NA <- apply(data, 2, function(x) sum(is.na(x)))
  
  prop.missing <- round(cnt.NA / nrow(data), 2)

  cnt.dupes <- apply(data, 2, function(x) sum(duplicated(x)))

  prop.duplicates <- round(cnt.dupes / nrow(data), 2)
  
  reportList <- data.frame(cbind(names(prop.missing), prop.missing, prop.duplicates))
  return(reportList)
}

quality.func(data) 

data <- data.frame(data)
attach(data)

```


## Distribution of variables {.flexbox .vcenter}

Most hostels were well rated, and while low ratings exist, many seem to be outliers

```{r, warning=FALSE, echo =FALSE}
colon <- c(6,8:14)
distribution <- data %>% gather("rating_type", "rating_value", c(6,8:14)) 
distribution %>% 
  ggplot(aes(x = rating_type, y = rating_value, fill = rating_type, alpha = 0.7)) +
  geom_violin() +
  geom_boxplot(width=0.1, fill="white", alpha = 1)+
  labs(title="Distribution of various ratings",x="Type Of Rating", y = "Ratings")

```

## Outliers

- We want to remove the most extreme outliers for our variables  
- The standard way to do this is to look at things that lie outside this range:

$$median \pm 1.5*IQR_{data}$$
- The variables in the data set were often skewed, especially ratings which were almost always high, so removing outliers was important for our analysis  
- However, one outlier for price was so absurd we investigated it further 

  + The 10000 USD stay?  
  
  + Through google searches, we found the hostels that cost over a million yen were because     of data entry errors.



## Price distribution|After Removing Outliers {.flexbox .vcenter}

```{r, echo = FALSE}
data %>% filter(price.from <10000) %>% 
  ggplot(aes(x = price.from)) +
  geom_density(fill = "green", color = "black") + 
  xlab("Lowest Price") + 
  ylab("Density") + 
  ggtitle("Density Plot of Prices for Japanese Hostels")#+ 
  #geom_boxplot(width=0.1, fill = "white")
```

## Correlation Table {.smaller .flexbox .vcenter}

```{r, echo = FALSE}
# subset 
only.num <- data %>% 
  select(price.from, summary.score,atmosphere, cleanliness, valueformoney, staff, facilities, security)

hostel.cor <- round(cor(only.num, use="complete.obs"), 2)

datatable(hostel.cor, caption = "Correlation Matrix: Variables of Interest", rownames =c('Lowest Price','Summary Score', 'Atmosphere', 'Cleanliness', 'Value for Price', 'Staff', 'Facilities', 'Security'),colnames = c('Lowest Price'=2,'Summary Score'=3, 'Atmosphere'=4, 'Cleanliness'=5, 'Value for Price'=6, 'Staff'=7, 'Facilities'=8, 'Security'=9 ))
```



## Part 2: One Sample Statistical Inference  
We are testing whether Hiroshima's sample mean of summary scores differs significantly from the our population parameter of interest, i.e. the population mean of Hiroshima's summary scores.  

Our hypotheses are as follows:  

$$ H_o: \mu_{scores} = 9.171429  $$

$$H_{a}: \mu_{scores} \neq 9.171429  $$

```{r, echo = FALSE}

#doing a t-test on Kyoto

mu.city <- mean(data$summary.score, na.rm = TRUE)

hiroshima <- filter(data, data$City =="Hiroshima")

hiroshima.mu <- mean(hiroshima$summary.score, na.rm = TRUE)

t.test(hiroshima$summary.score, mu = mu.city)

Cohen.d <- abs(mu.city - hiroshima.mu)/sd(data$summary.score, na.rm = TRUE)


# Cohen.d
#We set our H0: Hiroshima mean summary score = mu    HA: Hiroshima != mean summary score
#According to our conclusion, we reject the null hypothesis. Hiroshima mean of summary scores does not fall within the 95% confidence interval.
```
## Analysis of Test for Hiroshima's Summary Scores
- Our p-value of approximately 0.0001794 is lower than our alpha of 0.05, so we can reject our null  hypothesis. However, because our sample estimate falls within our 95% confidence interval, i.e. is not in agreement with our p-value, we would need to do further testing to be more confident in our results. 
- In context, this means that we can surmise that Hiroshima's summary scores differ from our population parameter of interest.  
- While we found statistically significant results, what about practical significance? 
  + Our cohen's d value is of a medium size
  + We can conclude that our test produced both significant results, both practically and statistically. 
```{r echo=FALSE}
print("Cohen's d:  0.4046792")
```

## Cities' confidence intervals for Mean Summary Score 

```{r, echo = FALSE, warning=FALSE}

mu.city <- mean(data$summary.score, na.rm = TRUE)
hiroshima <- filter(data, data$City =="Hiroshima")
hiroshima.mu <- mean(hiroshima$summary.score, na.rm = TRUE) 
h <- t.test(hiroshima$summary.score, mu = mu.city)
mu.city <- mean(data$summary.score, na.rm = TRUE)
osaka <- filter(data, data$City =="Osaka")
osaka.mu <- mean(osaka$summary.score, na.rm = TRUE) 
o <- t.test(osaka$summary.score, mu = mu.city)
mu.city <- mean(data$summary.score, na.rm = TRUE)
fukuoka <- filter(data, data$City =="Fukuoka-City")
fukuoak.mu <- mean(fukuoka$summary.score, na.rm = TRUE) 
f <- t.test(fukuoka$summary.score, mu = mu.city)
mu.city <- mean(data$summary.score, na.rm = TRUE)
kyoto <- filter(data, data$City =="Kyoto")
kyoto.mu <- mean(kyoto$summary.score, na.rm = TRUE) 
k <- t.test(kyoto$summary.score, mu = mu.city)
mu.city <- mean(data$summary.score, na.rm = TRUE)
tokyo <- filter(data, data$City =="Tokyo")
tokyo.mu <- mean(tokyo$summary.score, na.rm = TRUE) 
t <- t.test(tokyo$summary.score, mu = mu.city)
o <- c(8.407384, 8.846081)
f <- c(8.778739, 9.362438)
t <- c(8.562692,8.876652)
h <- c(9.009020, 9.333837)
k <- c(8.731997, 9.191291)
x <- unique(data$City)
x <- data.frame(x)
F <- c((8.407384 + 8.846081)/2,(8.778739 + 9.362438)/2,(8.562692+8.876652)/2,(9.009020+ 9.333837)/2,(8.731997+ 9.191291)/2 ) 
L <- c(8.778739,9.009020, 8.731997, 8.407384, 8.562692)
U <- c(9.362438, 9.333837,9.191291,8.846081,8.876652)
#require(plotrix)
#plotCI(x, y = NULL, ui=U, li=L)
#library(plyr)
x <- unique(data$City)
x <- data.frame(x)
thing<-data.frame(Group=factor(rep(c("Fukuoka-City","Hiroshima","Kyoto","Osaka","Tokyo"),2)),
                  
                                    
                  Y = c( 8.778739, 9.009020,
                        8.731997,8.407384,
                         8.562692,9.362438,
                        9.333837,9.191291,
                        8.846081,8.876652
                        
                        )
                  )
                  
                  
cdata <- ddply(thing, "Group", summarise,
               N    = length(Y),
               mean = mean(Y)
               )
#cdata
pd <- position_dodge(0.78)
ggplot(cdata, aes(x=Group, y = mean, group = Group)) +
   #draws the means
      geom_point(position=pd) +
   #draws the CI error bars
      geom_errorbar(data=cdata, aes(ymin=L, ymax=U, 
      color=Group), width=.1, position=pd)
```

## Part 3: Proprtion Test for Super Hostel Ratings  
For this test, our parameter of interest is the population proportion of superb rating bands for Japanese Hostels. We are testing whether the proportion of superb ratings in our sample differs significantly from population proportion.  
Our hypotheses are as follows: 
$$ H_{0}: p_{superb} =  0.5321637 $$
$$  H_{a}: p_{superb} \neq  0.5321637$$
```{r echo=FALSE}
test <- prop.test(182,342, alternative="two.sided" )
test
```
## Results of Hypothesis Test on Superb Ratings  
- For our test, we found that we cannot say that the sample proportion of superbly rated hostels differs significantly from our population parameter of interest.  
- Our sample estimate being within our confidence interval means that our confidence interval is in agreement with the results of our test.  
- Even though our results were not statistically significant, we evaluated the effect size as well, and found that the results of this test were practically insignificant as well due to the tiny effect size.   
```{r echo=FALSE}
effect.size<- 2* abs(asin(sqrt(182/342))-asin(sqrt(0.5321637)))
print("Effect Size:")
effect.size
```
## Confidence Interval for a Superb Hostel Rating 
The interpretation of the confidence interval: We can say with 95% confidence that all Japanese hostels on the website will have  47.9% - 58.5% of their ratings will be "Superb"

```{r, echo = FALSE}
alpha = 0.05
# Convert from string to factor
data.rating <- factor(data$rating.band)
# Find the number of obs
n <- length(data.rating)
# Find number of obs per type
rating.breakdown <- table(data.rating)
# Get the proportion
p_hat <- rating.breakdown['Superb']/n
# Calculate the critical z-score
z <- qnorm(1-alpha/2)
# Compute the CI
CI <- p_hat + c(-1,1)*z*sqrt(p_hat*(1-p_hat)/n)

paste("The 95% Confidence Interval is:", round(CI[1], 5), "to", round(CI[2], 5 ))
#interpretation of the confidence interval: We can say with 95% confidence that the all Japanese hostels on the website will have  47.9% - 58.5% of their ratings will be "Superb"
```



## Part 4: Two Sample Statistical Inference  
- For our two sample test, we considered the variable distance from city center.  
- Then we created considered whether certain attributes of the sample of hostels that are far from the city center is significantly different from those that are close to the city center. 
- Our first test is for the mean price, and our hypotheses are as follows: 
$$H_{0}:  $$ 
```{r, echo = FALSE, include=FALSE}
# subset for distance from city center 
mid.dist <- median(Distance)
#mean(Distance)

#two.test.value
# for price 
high.dist <- data %>% 
  select(Distance, price.from ) %>% 
  filter(Distance > mid.dist) 

low.dist <- data %>% 
  select(Distance, price.from) %>% 
  filter(Distance < mid.dist)

two.test.price <- t.test(high.dist$price.from, low.dist$price.from)
price.cohens <- cohen.d(high.dist$price, low.dist$price, pooled = TRUE)



# for summary score 

high.dist <- data %>% 
  select(Distance, summary.score ) %>% 
  filter(Distance > mid.dist) 

low.dist <- data %>% 
  select(Distance, summary.score) %>% 
  filter(Distance < mid.dist)

two.test.score <- t.test(high.dist$summary.score, low.dist$summary.score)

score.cohens <- cohen.d(high.dist$summary.score, low.dist$summary.score, pooled = TRUE, na.rm = TRUE)


# for value for money 

high.dist <- data %>% 
  select(Distance, valueformoney ) %>% 
  filter(Distance > mid.dist) 

low.dist <- data %>% 
  select(Distance, valueformoney) %>% 
  filter(Distance < mid.dist)

two.test.value <- t.test(high.dist$valueformoney,low.dist$valueformoney)


value.cohens <- cohen.d(high.dist$valueformoney, low.dist$valueformoney, pooled=TRUE, na.rm = TRUE)
```

```{r echo = FALSE}
two.test.price
price.cohens
```

## Next slide 
```{r echo = FALSE}
two.test.score
score.cohens
```

## Next slide, Again

```{r, echo = FALSE}
two.test.value
value.cohens
```

```{r, echo = FALSE}
# results of cohens effect size 
price.cohens
```

## Cohen's D for score

```{r, echo = FALSE}
score.cohens
```

#### Background for Test 
#### Hypotheses in Parametric Notation 
#### Two Sample Test 
#### Results and Analysis 


## Part 5: P(Value given Distance)

![](raw_data\PROBABILITY TREE.png)


```{r, echo = FALSE, include = FALSE}
data <-  data %>% filter(!is.na(valueformoney) & !is.na(Distance))

mid.dist <- median(data$Distance)

mid.score <- 6.5
hi.score <- data %>% 
  select(valueformoney) %>% 
  filter(valueformoney >= mid.score)
lo.score <- data %>% 
  select(valueformoney) %>% 
  filter(valueformoney < mid.score)

high.dist <- data %>% 
  select(Distance, valueformoney ) %>% 
  filter(Distance >= mid.dist) 

low.dist <- data %>% 
  select(Distance, valueformoney) %>% 
  filter(Distance < mid.dist)


# Dist. Counts 
set.size <- length(data$X)
hi.dist.cnt <- length(high.dist$Distance)
low.dist.cnt <- length(low.dist$Distance)
# Score Counts 
hi.score.cnt <- length(hi.score$valueformoney)
low.score.cnt <- length(lo.score$valueformoney)
# Counts for conditions 
# new sub 

hi.dist.score <- data %>% 
  select(Distance, valueformoney) %>%
  filter(Distance >= mid.dist & valueformoney > mid.score)
hi.dist.low.score <- data %>% 
  select(Distance, valueformoney) %>%
  filter(Distance >= mid.dist & valueformoney <= mid.score)
  
low.dist.score <- data %>% 
  select(Distance, valueformoney) %>%
  filter(Distance < mid.dist & valueformoney <= mid.score)
  
low.dist.hi.score <- data %>% 
  select(Distance, valueformoney) %>%
  filter(Distance < mid.dist & valueformoney > mid.score)
  
# Proportions Hi and low Distance
p.hi.dist <- hi.dist.cnt/set.size
p.low.dist <- low.dist.cnt/set.size
# Proportions Hi and low Scores 
p.hi.score <- hi.score.cnt/set.size
  
p.low.score <- low.score.cnt/set.size
# Sizes for Intersections of Probablilities of Dependent stuff 
hi.d.val <- length(hi.dist.score$valueformoney)
hi.d.lval <- length(hi.dist.low.score$valueformoney)
low.d.hval <- length(low.dist.hi.score$valueformoney)
low.d.val <- length(low.dist.score$valueformoney)
# Intersection Probablities
p.hd.hv <- hi.d.val/set.size
p.hd.lv <- hi.d.lval/set.size
p.ld.hv <- low.d.hval/set.size 
p.ld.lv <- low.d.val/set.size
# Because conditional prob = intersection/city center 
# hi dis hi val
cp1 <- p.hd.hv/p.hi.dist
# hi dis lo val 
cp2 <- p.hd.lv/p.hi.dist 
# low dis hi val 
cp3 <- p.ld.hv/p.low.dist 
# low dis low val 
cp4 <- p.ld.lv/p.low.dist
cp1 + cp2 + cp3 + cp4



```

### Conclusions 
#### Limitations  
#### Future Work and Questions  


